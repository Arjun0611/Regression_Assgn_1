{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ad52f7-8d45-4f13-909d-3fdf84b0ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1.\n",
    "\n",
    "# Simple Linear Regression:\n",
    "# Uses one independent variable.\n",
    "# Predicts one dependent variable.\n",
    "# Equation: y = b0 + b1(x) + Error. Here, b0 and b1 are coefficients and x is the independent variable.\n",
    "# Example: Predicting a student's score based on the number of hours they studied.\n",
    "\n",
    "# Multiple Linear Regression:\n",
    "# Uses two or more independent variables.\n",
    "# Predicts one dependent variable.\n",
    "# Equation: y = b0 + b1(x1) + b2(x2) + b3(x3) +...+ bn(xn) + Error. Here, b0, b1, b2, b3, etc. are coefficients and x1, x2, x3, etc. are independent variables.\n",
    "# Example: Predicting house price based on bedrooms, square footage, and location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3424cc78-d5e5-4790-bde6-d800c59b4b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2.\n",
    "\n",
    "# Assumptions of Linear Regression:\n",
    "\n",
    "# Linearity: The relationship b/w independent and dependent variables is linear.\n",
    "# Independence: Residuals are independent of each other.\n",
    "# Homoscedasticity: Residuals have constant variance.\n",
    "# Normality: Residuals are normally distributed.\n",
    "# No or little Multicollinearity: Independent variables are not higly correlated.\n",
    "\n",
    "# Checking Assumptions:\n",
    "\n",
    "# Residual Plot: Examine residuals vs predicted values for linearity.\n",
    "# Durbin-Watson Test: Tests for independence of residuals.\n",
    "# Residual Plot: Check for constant variance (homoscedasticity).\n",
    "# Normal Probability Plot: Assess normality of residuals.\n",
    "# Correlation Matrix: Examine correlations among independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24e63a4a-5a31-4966-9501-272d5af42d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3.\n",
    "\n",
    "# Slope (Coefficient of Independent Variable):\n",
    "\n",
    "# It represents how much the dependent variable changes for each unit change in the independent variable.\n",
    "# Positive slope indicates an increase in the dependent variable with an increase in the indepenedent variable and vice versa for negative slope.\n",
    "# It quantifies the impact and direction of the relationship.\n",
    "\n",
    "# Intercept:\n",
    "\n",
    "# Intercept is the value of the dependent variable when the independent variable is zero.\n",
    "# It's the starting point or baseline in the relationship.\n",
    "# Essential when the independent variable cannot be zero in practice.\n",
    "\n",
    "# Example:\n",
    "\n",
    "# Suppose we are analyzing the relationship between years of experience and salary for employees.\n",
    "# Slope: A positive slope indicates that for each additional year of experience, the salary increases by a specific amount.\n",
    "# Intercept: If the interecept is $70,000, it means that an employee with zero years of experience (e.g. a rookie) would start with a salary of $70,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "698f9b67-93ed-4460-a680-d371bc74fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4.\n",
    "\n",
    "# Gradient Descent is an optimization algorithm for finding the miniumum of a function.\n",
    "# Widely used in machine learning for training models.\n",
    "# Iteratively adjusts model parameters (e.g. slope and intercept in case of linear regression) to minimize the cost function.\n",
    "# Steps:\n",
    "#1. Start with initial parameters.\n",
    "#2. Calculate the gradient (derivative) of the gradient.\n",
    "#3. Update parameters in the opposite direction of the gradient.\n",
    "#4. Repeat until convergence.\n",
    "\n",
    "# It helps models learn from data by minimizing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56cc0c06-3657-4721-8100-2724342e96eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5.\n",
    "\n",
    "# Multiple Linear Regression:\n",
    "\n",
    "# Models the relationship b/w a dependent variable and two or more independent variables.\n",
    "# Equation: Y = b0 + b1(x1) + b2(x2) + b3(x3) +...+ bn(xn).\n",
    "# Aims to estimate coefficients (b0,b1,b2,...,bn) that fit the data.\n",
    "# Allows for capturing complex relationships using multiple predictors.\n",
    "# Coefficients represent the impact of each variable, holding others constant.\n",
    "# Evaluated using metrics like R-squared, mean squared error, etc.\n",
    "\n",
    "# Differences from Simple Linear Regression:\n",
    "\n",
    "# Multiple regression involves two or more independent variables.\n",
    "# It considers complex relationships with multiple predictors.\n",
    "# Interpretation can be more intricate due to interactions between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7b7cdc4-a097-422a-be64-2f181617ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6.\n",
    "\n",
    "# Multicollinearity in multiple linear regression:\n",
    "\n",
    "# Multicollinearity occurs when independent variables in a regression model are highly correlated with each other.\n",
    "# It makes it challenging to isolate the individual effects of each variable.\n",
    "# It can lead to unreliable coefficient estimates.\n",
    "\n",
    "# Detecting multicollinerity:\n",
    "\n",
    "# Check correlation matrices and correlation coefficients among predictors.\n",
    "# Variance inflation factor(VIF) can quantify the degree of multicollinearity.\n",
    "\n",
    "# Addressing multicollinearity:\n",
    "\n",
    "# Remove one of the correlated variables.\n",
    "# Combine correlated variables into a composite variable.\n",
    "# Regularization techniques like Ridge or Lasso regression can help mitigate multicollinearity.\n",
    "# Collect more data to reduce multicollinearity effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62fb5237-47ab-4d61-8921-dae18b4c0ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7.\n",
    "\n",
    "# Polynomial Regression:\n",
    "\n",
    "# Models nonlinear relationships.\n",
    "# Utilizes polynomial equations.\n",
    "# Incorporates higher-order terms.\n",
    "# Useful for curved data patterns.\n",
    "# Coefficients represent the term influences.\n",
    "# A type of multiple linear regression.\n",
    "\n",
    "# Ex: Predicting fuel efficiency based on speed with quadratic and cubic terms in the model.\n",
    "\n",
    "# Polynomial vs Linear Regression:\n",
    "# Non-linear vs Linear relationships.\n",
    "# Linear regression has only one predictor variable whereas polynomial regression has multiple predictor variables, including higher-order terms.\n",
    "# Linear regression has constant slope and intercept whereas polynomial regression has varying slopes and intercepts.\n",
    "# Linear regression fits straight lines vs polynomial regression fits curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e405667d-50fa-45ea-934b-08f46a11b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8.\n",
    "\n",
    "# Advantages:\n",
    "\n",
    "# Captures nonlinear relationships.\n",
    "# More flexible can fit complex data patterns.\n",
    "# Provides a better fit for curvilinear data.\n",
    "\n",
    "# Disadvantages:\n",
    "\n",
    "# Prone to overfitting with high-degree polynomials.\n",
    "# Can lead to unstable predictions outside the data range.\n",
    "# Interpretation becomes more complex as degree increases.\n",
    "\n",
    "# When to use Polynomial Regression:\n",
    "\n",
    "# The data exhibits a curvilinear pattern.\n",
    "# Linear regression fails to capture the relationship.\n",
    "# Cautious about overfitting and control polynomial degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "358cd3e1-93e1-4181-ad56-3f7de63829d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d117f530-abb7-4d53-8d6f-a50ce80741c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a4d3deb-91c3-4131-84b9-d22f6b81792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f323be-c8b9-4761-a62b-45883efa3480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
